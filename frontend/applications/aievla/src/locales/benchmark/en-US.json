{
  "benchmark": {
    "menu": {
      "list": "Leaderboard",
      "config": "Benchmark config",
      "task": "Benchmark task",
      "indicator": "Evaluation indicators",
      "effectEvaluation": "Effect Evaluation",
      "performanceEvaluationTask": "Performance Evaluation Task",
      "evaluationRules": "Evaluation Rules"
    },
    "config": {
      "configTab": "Configuration",
      "namePlaceholder": "Please enter a rule name",
      "createConfig": "Create Rule",
      "copyConfig": "Copy Rule",
      "editConfig": "Edit Rule",
      "configName": "Rule name",
      "taskCount": "Task number",
      "relationDataSetCount": "Associated evaluation dataset number",
      "relationDataSet": "Associated evaluation dataset",
      "relationIndicatorCount": "Associated metric number",
      "relationIndicator": "Associated metric",
      "relationDataSetName": "Associated evaluation dataset name",
      "relationIndicatorName": "Associated metric name",
      "relationTaskName": "Associated task name",
      "createTip": "Click the |【Create 】| to add a rule",
      "searchPlaceholder": "Search rule name",
      "inputDes": "Please enter a description",
      "clearConfig": "Clear rules",
      "addDataset": "Add evaluation dataset file",
      "addMetric": "Add metric",
      "exitTips": "The current content has not been saved, and the configured content will not be restored after exiting, please operate with caution.",
      "deleteConfigTips": "Once the benchmark task is deleted, the rule cannot be retrieved, so please operate with caution.",
      "taskInfo": "Task information",
      "taskName": "Task name",
      "taskNamePlaceholder": "Please enter a task name",
      "taskErrorTip": "Please complete the task node rule first before proceeding with subsequent rule.",
      "datasetFieldName": "Choose evaluation dataset and version",
      "datasetPlaceholder": "Please choose evaluation dataset and version",
      "datasetTooltip1": "Select at least one input and output per file. Evaluation dataset input is the sample provided to the algorithm for training or prediction; Evaluation dataset output refers to the target value of the data to be predicted.",
      "datasetInputTooltip": "Inputs that require algorithmic calculations",
      "datasetOutputTooltip": "The ideal value for the algorithm calculation result. Metric takes the actual output of the algorithm and the ideal result as input items to calculate the metric results to evaluate the performance of the algorithm.",
      "configInputsOutPuts": "Configure evaluation dataset input and output",
      "datasetEmptyFileErrorTip": "Configure at least one file",
      "datasetErrorTip": "The file must contain an input and output",
      "dataName": "Field name",
      "previewData": "Preview data",
      "datasetFileEmptyTip": "The evaluation dataset file is empty, please check the file information",
      "datasetFileErrorTip": "evaluation dataset file parsing failed, please check related information",
      "deleteTaskNodeTip": "Once the benchmark task is deleted, the configuration cannot be retrieved, so please operate with caution.",
      "clearConfigTipTitle": "Confirm clear configuration？",
      "clearConfigTipContent": "Once cleared the benchmark configuration cannot be retrieved.",
      "deleteDatasetNodeTip": "Once deleted the evaluation dataset, metric, and adapter configuration content cannot be retrieved, so please operate with caution.",
      "indicator": "metric",
      "goToIndicator": "Go to evaluation indicators",
      "adapterDesc": "Adapter is used to adapt the input and output formats of evaluation dataset and indicators so that data can flow smoothly. Please view the data flow | diagram |. When configuring the evaluation dataset, select multiple outputs. You need to adjust the adapter sample template content and upload it for data flow.",
      "averageTooltip": "The average score in the benchmark list is composed of the average value of the metric selected in the configuration.",
      "saveErrorTips": "Please correct the configuration item error.",
      "selectFileTip": "Please select the file and preview related data",
      "datasetEmptyTip": "The current evaluation dataset version is empty",
      "datasetConfigTips": "Add at least one evaluation dataset file",
      "indicatorConfigTips": "Add at least one metric",
      "configTitle": "Configuration information",
      "configSubTitle": "Different configurations are used to evaluate and compare the performance of different algorithms.",
      "taskSubTitle": "Fill in the basic information of the configuration task to distinguish the goals of different configuration tasks",
      "datasetSubTitle": "Configure the column fields of the evaluation dataset to be associated with the indicator input fields to provide ground truth for indicator calculations.",
      "selectFileTips": "Please select file",
      "datasetFileErrorTips": "The evaluation dataset file parsing failed. Please check the relevant information or switch the file and configure it.",
      "datasetFileEmptyTips": "The evaluation dataset file is empty, please check the file information or switch the file and configure it.",
      "datasetPlaceholder2": "Please select the evaluation dataset and version before configuring",
      "indicatorSubTitle": "Based on the evaluation dataset, select the corresponding index of the performance of the algorithm.",
      "indicatorNotFound": "Metric not found?",
      "deletePopConfirmTitle": "Are you sure you want to remove this evaluation dataset",
      "type": "New type",
      "blankConfig": "Blank configuration",
      "templateConfig": "Template configuration",
      "template": "Template",
      "datasetExampleTitle": "Evaluation dataset example for template",
      "diagram": "Diagram",
      "Illustration": "Illustration",
      "IllustrationStart": "evaluation dataset output",
      "IllustrationMiddle": "Converted by Adapter",
      "IllustrationEnd": "As input to the indicator",
      "desc": "The adapter converts the format of the output of the evaluation dataset and adapts it to the format of the indicator.",
      "explanation": "Explanation",
      "example": "Example",
      "exampleTitle": "Evaluation dataset file evaluation data_1",
      "exampleDesc": "In the evaluation dataset configuration, select query as the input and Positive Document as the output. After the format conversion of the adapter, it is used as the input of the indicator. The format is converted to:",
      "leaderboard": "Leaderboard",
      "leaderboardSubTitle": "After the evaluation is completed, the results of the evaluation",
      "totalAverageTip": "[Not configured yet, the switch is turned on, and this average appears in the list]",
      "outMetricPlaceholder": "Please select output metric",
      "outMetricRequireTip": "Please complete the leaderboard configuration",
      "displayParam": "Default display parameters",
      "divideEqually": "Divide equally",
      "task": "Task",
      "templatePlaceholder": "Please select a template",
      "noMetric": "No metric data yet",
      "noConfigNodeTips": "Your configuration has not been completed yet. Please complete the following configuration before publishing.",
      "noDataset": "Evaluation dataset does not exist",
      "datasetNoPermission": "No permissions are currently available to view the contents of the evaluation dataset",
      "publishConfigTitle": "Are you sure you want to publish the Benchmark configuration?",
      "publishConfigContent": "After the Benchmark configuration is published, it cannot be edited again.",
      "saveSuccess": "Saved successfully, you can go to \"Benchmark Task\" to use this configuration to execute the task.",
      "helpGuide": "Help guide",
      "notTip": "Don't prompt again",
      "tip": {
        "one": {
          "title": "1. Configuration information",
          "item1": "Edit the configuration name, description and color in the sidebar",
          "item2": "Hover the canvas configuration information node and click the | on the right side of it to add a task"
        },
        "two": {
          "title": "2. Task information",
          "item1": "Edit task name and description in the sidebar",
          "item2": "Hover the task information node on the canvas and click the | on the right side of it to add evaluation dataset and indicators."
        },
        "three": {
          "title": "3. Evaluation dataset",
          "item1": "Click | Add Evaluation dataset File",
          "item2": "Select the evaluation dataset version in the sidebar",
          "item3": "Select the file under this version",
          "item4": "Configure evaluation dataset file input and output"
        },
        "four": {
          "title": "4. Metrics",
          "item1": "Click | Add Metric",
          "item2": "Check the required metric",
          "item3": "Hover the metric and click 【View metric details】 to learn about the metric information"
        },
        "five": {
          "title": "5. Adapter",
          "item1": "| Upload the Adapter file on demand",
          "item2": "View 【Image】 and download 【Sample Template】 to understand the specific use of Adapter"
        },
        "six": {
          "title": "6. Leaderboard",
          "item1": "Select the output indicators of each evaluation dataset node. If the evaluation dataset node selects multiple output indicators, they will be displayed in the overall list according to the average distribution of the selected indicators.",
          "item2": "Configure the total average score or task average score in the list as needed"
        }
      },
      "clearFileConfig": "Clear selection",
      "clearDatasetConfig": "Clear evaluation dataset",
      "clearLeaderboard": "Clear leaderboard",
      "relationship": "Relationship"
    },
    "indicator": {
      "name": "Metric name",
      "namePlaceholder": "Please enter the metric name",
      "type": "Metric type",
      "searchPlaceholder": "Search metric name",
      "createTip": "Click the|【Create 】|to add a metric",
      "createIndicator": "Add metric",
      "viewIndicator": "View metric",
      "editIndicator": "Edit metric",
      "fileFormatErrorTip": "File format error, only supports py",
      "fileSizeErrorTip": "The size of a single file should not exceed 2M",
      "uploadFileTip1": "Support upload a py format file, and the file size cannot exceed 2M.",
      "uploadFileTip2": "Click |【sample template】|to download",
      "indicatorAnalysis": "Metric analysis",
      "indicatorParam": "Parameter metric",
      "paramName": "Parameter Name",
      "paramDesc": "Parameter Description",
      "deleteTitle": "Confirm deletion？",
      "deleteTip": "Once deleted, the metric cannot be retrieved, so please proceed with caution.",
      "addFileTip": "Click the|【Select a file】|to add and preview metric data",
      "fileEmptyTip": "The file data is empty, please check the file information",
      "fileErrorTip": "File parsing failed, please check related information",
      "neiZhi": "built-in",
      "customize": "customize",
      "filePreview": "File preview",
      "noFileTip": "Please upload files",
      "fileErrorTip2": "File upload error, please upload the file again",
      "fileErrorTip3": "The file data is empty, please upload again",
      "repeatName": "Name already exists. Please modify the file information and upload it.",
      "noNull": "Name is not allowed to be empty. Please modify the file information and upload it.",
      "onlyKeyboard": "Only support Chinese and English, numbers and special characters on the keyboard. Please modify the file information and upload it.",
      "lenErr": "Enter up to 50 characters. Please modify the file information and upload it.",
      "fileAnalysing": "File is being parsed",
      "desc": "Instructions for use",
      "noDesc": "No instructions for use yet",
      "dir": "Director",
      "fileRequireDesc": "Description of document requirements",
      "funcTip": "Using specific quantitative indicators (such as keyword matching rates, ROUGE scores, etc.) to evaluate the performance of performance of models, customize applications, etc . These indicators are usually based on predefined standards and algorithms and can provide objective numerical results, but may not fully reflect the actual application of the evaluation object .",
      "aiTip": "Uses advanced language models to simulate the role of human assessors and automatically score the quality of answers. It aims to provide a fast and automated assessment method while maintaining a certain degree of accuracy and objectivity.",
      "llm": "LLM",
      "prompt": "Prompt",
      "func": "Function calculation",
      "ai": "AI evaluation ",
      "rule": "Calculation rules ",
      "selectModelTip": "The parameter quantity of a large model will affect the score calculation effect. It is recommended to choose a large model with a parameter quantity above 72B.",
      "totalSelected": "Total selected",
      "clearSelected": "Clear selected",
      "viewDetails": "Check the details"
    },
    "task": {
      "task": "Task"
    },
    "guide": {
      "title": "Benchmark operation guide",
      "flow1": {
        "title": "Prepare evaluation dataset",
        "subTitle": "Prepare evaluation dataset for evaluating algorithm capabilities"
      },
      "flow2": {
        "title": "Upload evaluation metrics",
        "subTitle": "Upload standards used to measure and evaluate algorithm effectiveness and performance"
      },
      "flow3": {
        "title": "Create benchmark configuration",
        "subTitle": "Associate evaluation dataset with indicators to provide benchmarks for indicator evaluation"
      },
      "flow4": {
        "title": "Configure benchmark tasks",
        "subTitle": "Bind the benchmark configuration (measurement standard) to the algorithm and define specific benchmark tasks"
      },
      "flow5": {
        "title": "Leaderboard",
        "subTitle": "View the indicator evaluation report after training is completed"
      }
    }
  }
}
