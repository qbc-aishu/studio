{
  "benchmarkTask": {
    "searchPlace": "Search for task name",
    "benchmarkTask": "Benchmark Task",
    "BenchmarkConfig": "Evaluation Rules",
    "algType": "Algorithm Type",
    "evalObjType": "Evaluation object type",
    "addEvaObjType": "Add evaluation object",
    "delEvaObjType": "Delete evaluation object",
    "chooseAPromptWord": "Please choose a prompt word first",
    "var": "Variable",
    "promptTip": "Variables are placeholders that need to be assigned different types and values in prompts. By selecting the generate variable button in the input box or clicking the [\n variable ] button of the variable list, the variable will become {{input}} in the prompt word input box, and the variable list will synchronize the variable.",
    "prompt": "Prompt",
    "required": "Required",
    "fieldName": "Display name \n The name displayed during debugging and preview is consistent with the variable name by default",
    "varName": "Variable name",
    "atLeastOnePrompt": "At least one Prompt word",
    "evalObj": "Evaluation object",
    "LModel": "Large Model",
    "LModelTwo": "large model",
    "SModel": "Small Model",
    "SModelTwo": "small model",
    "customApp": "Customised App",
    "customAppTwo": "customised app",
    "external": "External Integration",
    "externalTwo": "external integration",
    "pleaseInutExternal": "Please enter the name of the evaluation object",
    "pleaseInutURL": "Please enter URL",
    "status": "Status",
    "partialException": "Partial Exception",
    "terminated": "Terminated",
    "inProgress": "In Progress",
    "waiting": "Waiting",
    "notrun": "Not Run",
    "clear": "Clear",
    "taskName": "Task Name",
    "run": "Run",
    "color": "Color",
    "termination": "Termination",
    "viewTask": "View task configuration",
    "viewResult": "View Results",
    "createTask": "Create new task",
    "selectAlg": "Selection Algorithms",
    "selectEvalObj": "Selection evaluation objects",
    "algtooltip": "After the evaluation object is run, its results will overwrite the results of the evaluation object with the same name in the list.",
    "add": "Add an algorithm",
    "addEvalObj": "Add an evaluation object",
    "algorithm": "Algorithm",
    "algName": "Algorithm Name",
    "evalObjName": "Evaluation object name",
    "adapterFile": "Adapter files",
    "adaptertooltip": "Adapter files are used to adapt the input of the evaluation dataset and algorithm, as well as the output and metric function of the algorithm.",
    "adapterTitle": "Adapter is used to adapt the input and output formats of evaluation dataset, evaluation objects, and indicators so that data can flow smoothly. When the Adapter is not uploaded, the built-in Adapter will be used during task execution. However, in the following scenarios, the built-in Adapter cannot meet the needs and needs to be modified and uploaded manually:",
    "adapterTip": "1. In the selected Benchmark configuration, multiple files are selected for one evaluation dataset.|2. When the evaluation object selects a small model, the selected Benchamrk configured evaluation dataset file selects multiple inputs.|3. When the evaluation object selects a small model, the input of the evaluation object is not the value corresponding to the input.|4. Select the evaluation object for external access.|5. Non-built-in indicators are used in the selected Benchmark configuration.",
    "selectFile": "Please select a file",
    "uploadExtra1": "File format requires python",
    "uploadExtra2": "Supports uploading multiple files, the size of a single file should not exceed 2M",
    "uploadExtra3": "Click to download | [sample template]",
    "previewFailed": "File preview failed",
    "fileError": "File format error, only supports py",
    "fileSizeError": "The size of a single file should not exceed 2M",
    "fcontentError": "Incorrect format, please check your file",
    "adapterError": "Adapter file upload error",
    "exitTitle": "Are you sure to quit?",
    "exitContent": "Current configuration not saved. The configured content will not be retrived if you abandon the save. Please exercise caution when performing this operation!",
    "deleteTitle": "Are you sure to  delete the configuration?",
    "deleteContent": "Once the task is deleted, it cannot be retrieved. Please exercise caution when performing this operation!",
    "emptyText": "Click | [Create] | button to add tasks",
    "noResult": "No results available yet",
    "afterRunTip": "The task is running. You can go to \"Benchmark-Benchmark Task\" or \"Leaderboard\" to view it later.",
    "sortByName": "Sort by name",
    "rank": "Ranking",
    "publisher": "Publisher",
    "publishDate": "Release Date",
    "leaderboard": "Leaderboard",
    "searchLeaderboard": "Search evaluation object/publisher",
    "statusInfo": "Status Details",
    "noData": "No data",
    "stop": "Task terminated successfully",
    "taskRun": "Task runs successfully",
    "noPublish": "There are no published custom apps yet",
    "agentEmpty": "There is no released Agent yet.",
    "showPublished": "(Only the task results of published Benchmark configurations are displayed)",
    "boardTip": "Only the task results of published Benchmark configurations are displayed in the leaderboard. You can go to the Benchmark task, select any task in the corresponding/list, and click \"Operation > View Results\" to view the results.",
    "log": "View Log",
    "runDetail": "Run details",
    "noLog": "No log",
    "comprehensive": "comprehensive",
    "addMost": "Add up to 10 algorithms",
    "noPermission": "The custom application does not have permission or has been deleted",
    "noPermissionCustom": "The custom application does not exist or has no permissions.",
    "failDetail": "Cause of failure",
    "searchAgentPlaceholder": "Search for Agent Name",
    "searchCustomPlaceholder": "Search for custom app names ",
    "searchSmallModelPlaceholder": "Search for small model names ",
    "taskDelete": "Are you sure you want to delete \"|\" Benchmark task? After deletion, the Benchmark task cannot be retrieved, so please operate with caution.",
    "pleaseSelect": "Please select a {name}",
    "startRun": "Start running",
    "closeRun": "Shutdown run",
    "selectPrompt": "Please select prompt word",
    "inputPrompt": "Please input prompt word",
    "searchPromptName": "Search prompt word name",
    "promptNameLow": "prompt word",
    "runResult": "Running results",
    "runStatus": "Running status",
    "rerun": "Rerun",
    "runTasks": "Run tasks",
    "viewChecked": "View checked items",
    "emptyTip": "There are currently no evaluation objects to choose from. If you need to evaluate {type}, please go to {path} to complete the configuration first",
    "modelPath": "\"Model Factory>LLM\"",
    "agentPath": "\"Agent Factory>Agent\"",
    "smallPath": "\"Model Factory>Small Model Management\"",
    "customPath": "\"Cognitive Application>Customised Apps\"",
    "noExistErrorTip": "The {name} does not exist or has no permission",
    "EvaluationNotEmty": "Evaluation object is not allowed to be empty",
    "restartRunTip": "There is a task currently in progress, would you like to stop it and start a new task?",
    "draft": "Draft",
    "unpublishedChanges": "Modify unpublished",
    "published": "Published"
  }
}
